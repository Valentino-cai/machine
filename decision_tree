main函数：
    数据导入df
    训练集(train)和测试集(test)的划分
    利用Treegenerate函数进行树的生成
        root=Treegenerate(train)
        
结构体函数，建立树节点的属性
class Node(object):...
    树的每个节点有以下内容：1.该节点的属性 2.该节点的下一个节点属性 3.标签（？）
    
Treegenerate(df)树的生成函数:             #树的生成是节点不断递归调用Treegenerate函数的过程
    先生成一个node，其中内容为空None {}  None
    给出退出递归的条件
        1.如果df中所有样本的标签都相同，则无需划分
            使用Labelcount函数，将df的标签行传入，返回不同的标签的数据数量的字典
            如果返回的函数的长度为1，则说明所有样本的标签都相同,递归返回node
        2.如果df中所有样本在所有属性上的取值都相同（可能出现同义词）或者当前样本的属性集合为空，则无法划分
        3.如果当前节点没有样本，则不能划分
            如果上述返回的函数的长度为0，则说明当前样本集为空
    利用Optattr函数得到最优属性值  返回值包括最优分类属性，或者连续值的最优分类值
            
 Labelcount(df)不同标签数据数量统计函数
    使用for循环，依次统计标签属性数量
    返回字典
  
 Optattr(df) 最优属性或连续最优分类值选择函数
    使用for循环
    利用Infogain函数获得每个属性的的信息增益  传入的信息为列属性名和df
    
Infogain(attr,df) 信息增益计算函数
    利用Ent函数计算标签行的信息熵
    若attr对应的属性值为数值
        
    若attr对应的属性值为字符
        利用Labelcount函数统计attr列的属性值数据字典
        统计一共还有多少个样本
        利用for循环属性值attrtry，以Labelcount返回值（字典返回，带入for语句，使用的是key）
            选出attrtry对应的标签，选出形成dataframe
            infogain_total-=attrtry在labelcount中对应的数量/剩余样本数*Ent(产生的dataframe)
       
    
Ent(attrlist)信息熵函数
 
    

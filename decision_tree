main函数：
    数据导入df
    循环多次重复
        训练集(train)和测试集(test)的划分
        利用Treegenerate函数进行树的生成
            root=Treegenerate(train)
            利用Predict函数进行预测
            并统计准确率
 
Predict(root,df):预测函数，返回例子df的标签
     若root的属性为空，则退出循环
        若当前root的属性值为数值
            根据root的属性值的大小
            将root判断如下一个节点
        若当前root的属性为字符串
            根据root的属性值
            root等于其下一个节点



结构体函数，建立树节点的属性
class Node(object):...
    树的每个节点有以下内容：1.该节点的属性 2.该节点的下一个节点属性 3.标签（？）
    
Treegenerate(df)树的生成函数:             #树的生成是节点不断递归调用Treegenerate函数的过程
    先生成一个node，其中内容为空None {}  None
    给出退出递归的条件
        1.如果df中所有样本的标签都相同，则无需划分
            使用Labelcount函数，将df的标签行传入，返回不同的标签的数据数量的字典
            如果返回的函数的长度为1，则说明所有样本的标签都相同,递归返回node
        2.如果df中所有样本在所有属性上的取值都相同（可能出现同义词）或者当前样本的属性集合为空，则无法划分
        3.如果当前节点没有样本，则不能划分
            如果上述返回的函数的长度为0，则说明当前样本集为空
    利用Optattr函数得到最优属性值  返回值包括最优分类属性，或者连续值的最优分类值
    若返回数值属性
        根据返回的最好分类数值，进行二分，产生左右数据集
        并利用新的左右数据集递归继续生成树
    若返回字符串属性
      属性循环
        利用.isin选出属性值所在的行，形成dataframe
        从dataframe中删除该属性列
        递归产生生成树
       
       
 Labelcount(df)不同标签数据数量统计函数  还用于分类
    使用for循环，依次统计标签属性数量
    返回字典
  
 Optattr(df) 最优属性或连续最优分类值选择函数
    使用for循环
    利用Infogain函数获得每个属性的的信息增益  传入的信息为列属性名和df
    
Infogain(attr,df) 信息增益计算函数
    利用Ent函数计算标签行的信息熵
    若attr对应的属性值为数值
        按照所选属性进行整体排序
        重新进行编号
        依次循环，将两个相邻值的平均值作为划分值
        求出平均值前后所有数值对应行的信息熵之和
    infogain_total-=最小的那个信息增益
    若attr对应的属性值为字符
        利用Labelcount函数统计attr列的属性值数据字典
        统计一共还有多少个样本
        利用for循环属性值attrtry，以Labelcount返回值（字典返回，带入for语句，使用的是key）
            选出attrtry对应的标签，选出形成dataframe
            infogain_total-=attrtry在labelcount中对应的数量/剩余样本数*Ent(产生的dataframe)
       
    
Ent(attrlist)信息熵函数
    先用Labelcount完成统计分类
    然后分类计算信息熵
    
